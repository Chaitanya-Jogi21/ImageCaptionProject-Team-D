Image Captioning Project using BLIP  
Team-D | Major Project

An AI-powered system that automatically generates meaningful captions for images using the **BLIP (Bootstrapping Language-Image Pre-training)** model from HuggingFace.  
This project integrates **Flask**, **Python**, **Transformers**, and a clean web interface to help users upload an image and instantly generate a high-quality caption.

---

## ğŸ“Œ Table of Contents
1. Project Overview  
2. Why BLIP Instead of CNN + LSTM?  
3. Objectives  
4. Features  
5. Architecture  
6. Tech Stack  
7. Project Structure  
8. Installation & Setup  
9. How to Run  
10. Output Examples  
11. Screenshots  
12. Team Members  
13. Future Enhancements  
14. License  

---

## ğŸ“Œ 1. Project Overview
Image captioning is the task of generating a textual description from an image.  
Traditionally, this is achieved using:

- **CNN** â†’ for feature extraction  
- **LSTM** â†’ for sequence generation  

However, modern Vision-Language Models like **BLIP** outperform the old approach and provide near human-level caption generation.

In this project, BLIP acts as both the **encoder** and **decoder**, eliminating the need to manually build CNN + LSTM pipelines.

---

## ğŸ“Œ 2. Why BLIP Instead of CNN + LSTM?
Although the project report mentions CNN + LSTM, the team selected BLIP for the following reasons:

### âœ” BLIP Advantages  
- **Already trained on millions of imageâ€“text pairs**  
- **More accurate than CNN + LSTM**  
- **Faster caption generation**  
- **Does not require huge datasets for training**  
- **Uses transformer-based architecture**  

 âœ” Conceptually the Same Workflow  
- BLIP's image encoder â‰ˆ CNN  
- BLIP's text decoder â‰ˆ LSTM  

So even though we didnâ€™t manually build CNN + LSTM,  
**the underlying encoder-decoder process remains the same**, just upgraded with modern AI.

---

## ğŸ¯ 3. Objectives
- Generate meaningful captions for images.  
- Use a modern deep learning model for captioning.  
- Create a simple and elegant web interface.  
- Deploy the system with Flask.  
- Learn practical AI model integration.  

---

## â­ 4. Features
- Upload any image  
- Generate accurate captions  
- Clean, responsive UI  
- Fast processing  
- Uses state-of-the-art BLIP model  
- Easy to extend or deploy  

---

## ğŸ§  5. Architecture (High-Level)
        User Uploads Image
             â†“
 Flask Backend Receives Image
             â†“
 BLIP Processor Extracts Features
             â†“
BLIP Model Generates Caption (Decoder)
             â†“
   Caption Returned to Frontend
             â†“
     User Sees Final Output

---

## ğŸ› ï¸ 6. Tech Stack

| Category | Technology |
|---------|-------------|
| **AI Model** | BLIP (HuggingFace Transformers) |
| **Backend** | Python, Flask |
| **Frontend** | HTML5, CSS3 |
| **Libraries** | torch, transformers, pillow |
| **Version Control** | Git & GitHub |

---

## ğŸ“‚ 7. Project Structure

image-caption-project/
â”‚â”€â”€ app.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html
â”‚
â””â”€â”€ static/
â””â”€â”€ style.css


---

ğŸ”§ 8. Installation & Setup
Step 1 â€” Clone the Repo 
Step 2 â€” Install Dependencies  
Step 3 â€” Run Flask App  
Step 4 â€” Open Browser  

---

## â–¶ï¸ 9. How to Use  
1. Run the project  
2. Go to the website  
3. Upload any image  
4. Click **Generate Caption**  
5. Model returns a meaningful caption  

---

## ğŸ“¸ 10. Output Examples  
Input Image â†’ A dog in a field
Model Output â†’ "a dog running through a grassy area"

Input Image â†’ A bowl of fruits
Model Output â†’ "a bowl filled with apples and bananas"

## ğŸ‘¥ 12. Team Members:-
Chaitanya Jogi
Deepak Choudhary
Manoj kumar Yanamadala
Mohd Mudabbir Arafat
Siva Adapa
Anantha Sathish Kumar Palchuri
Sowmiya S
Syed Hasanuddin
Kukatla Kamal

---

## ğŸš€ 13. Future Enhancements
- Add voice narration  
- Deploy on cloud (AWS/Render)  
- Add dataset-based training module  
- Add attention visualization  
- Multi-language caption output  
- Add image-to-story generator  

---

## ğŸ“œ 14. License  
This project is developed for academic purposes by **Team-D**.  
Feel free to fork and improve it.

---

